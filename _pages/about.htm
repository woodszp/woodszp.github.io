<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/">
<link rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Wenbin Li's Homepage (Nanjing University)</title>
<style type="text/css">
</style></head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>Wenbin Li @ R&L Group</h1>
</div>
<table class="imgtable"><tbody><tr><td>
<a href="lwb4.jpg"><img src="lwb4.jpg" alt="lwb4.jpg" width="240px" height="307px"></a>&nbsp;</td>
<td align="left"><p><b><font size="+3" face="Times New Roman">Wenbin Li</font> <font size="+3" face="华文楷体">(李文斌)</font></b><br>
<font size="+2" face="Times New Roman">Associate Researcher</font> <font size="+2" face="华文楷体">(副研究员)</font><br><br>
<a href="https://cs.nju.edu.cn/rl/index_eng.htm">R&L Group</a><br>
<a href="http://cs.nju.edu.cn/">Department of Computer Science and Technology</a> <br>
<a href="http://keysoftlab.nju.edu.cn/site/ndjsjx/">State Key Laboratory for Novel Software Technology</a> <br>
<a href="http://www.nju.edu.cn/">Nanjing University</a><br><br>
Email: <a href="mailto:liwenbin@nju.edu.cn">liwenbin@nju.edu.cn</a>; <a href="mailto:liwenbin.nju@gmail.com">liwenbin.nju@gmail.com</a><br>
Office: Room 1019, Computer Science Building, Xianlin Campus of Nanjing University, Nanjing 210023, China</p>
[<a href="https://scholar.google.com/citations?user=K-kC4yYAAAAJ&hl=zh-CN&authuser=1"><span style="color:purple">Google Scholar</span></a>] [<a href="https://github.com/WenbinLee"><span style="color:purple">Github</span></a>]
[<a href="https://github.com/RL-VIG"><span style="color:purple">Github-VIG</span></a>]
</td>

<td valign="top" width="236"><a href="http://cs.nju.edu.cn/rl/index_eng.htm" target="_blank"><img height="70" src="rlgroup.jpg" width="236" border="0"></a></td>
<td valign="top" width="58"><a href="http://www.nju.edu.cn/" target="_blank"><img height="70" src="nju.jpg" width="58" border="0"></a></td></tr></tbody></table>


<h2>Biography</h2>
<p>
Currently, I am an Associate Researcher of <a href="http://cs.nju.edu.cn/" target="_blank">Department of Computer Science and Technology</a> at
      <a href="http://www.nju.edu.cn/" target="_blank">Nanjing University</a> and a member of<br> 
      <a href="https://cs.nju.edu.cn/rl/" target="_blank">Reasoning and Learning Research Group</a>, led by professor <a href="https://cs.nju.edu.cn/gaoyang">Yang Gao</a>.<br>

<span class="norm"><br class="style1"></span>I received my Ph.D. degree in <a href="http://cs.nju.edu.cn/" target="_blank">Department of Computer Science and Technology</a> in December 2019 from <a href="https://www.nju.edu.cn/EN/">Nanjing University</a>.
<span class="norm"><br class="style1"></span>I received my B.Sc. degree in <a href="http://cs.cumt.edu.cn/">School of Computer Science and Technology</a> in June 2013 from <a href="http://http://www.cumt.edu.cn//">China University of Mining and Technology</a>.
<span class="norm"><br class="style1"></span>I visited the <a href="http://www.cs.rochester.edu/u/jluo/#VISTA">VIStA</a> 
group from December 2017 to June 2019 at the <a href="https://www.rochester.edu">University of Rochester</a>, under the supervision of Prof. <a href="http://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a>.
</p>



<h2>News</h2>
<ul>
<li><p><b><font face="Times New Roman">[04/2023]</font></b>: <a href="https://mp.weixin.qq.com/s/fdw616P_ygZ0LnTf-QdSwQ"><font size="+1" face="华文楷体">课题组招收2023年的博士研究生、硕士研究生，欢迎与我们联系!</font></a></p>
</li>
<li><p><b><font face="Times New Roman">[03/2023]</font></b>: One paper on “Novel class discovery” is accepted to <a href="https://cvpr2023.thecvf.com/">CVPR 2023</a>.</p>
</li>
<li><p><b><font face="Times New Roman">[11/2022]</font></b>: One paper on “Multi-agent reinforcement learning” is accepted to <a href="https://aaai.org/Conferences/AAAI-23/">AAAI 2023</a>.</p>
</li>
<li><p><b><font face="Times New Roman">[10/2022]</font></b>: One paper on “Few-shot learning” is accepted to <a href="https://ieeexplore.ieee.org/document/9916072">TPAMI 2022</a>.</p>
</li>
<li><p><b><font face="Times New Roman">[10/2022]</font></b>: One paper on “Online metric learning” is accepted to <a href="https://ieeexplore.ieee.org/document/9928295">TNNLS 2022</a>.</p>
</li>
<li><p><b><font face="Times New Roman">[07/2022]</font></b>: One paper on “Few-shot learning” is accepted to <a href="https://eccv2022.ecva.net/">ECCV 2022</a>.</p>
</li>
<li><p><b><font face="Times New Roman">[05/2022]</font></b>: One paper on “Caricature generation” is accepted to <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9765839">TIP 2022</a>.</p>
</li>
<li><p><b><font face="Times New Roman">[05/2022]</font></b>: One paper on “Online learning” is accepted to <a href="https://ieeexplore.ieee.org/document/9791447">TNNLS 2022</a>.</p>
</li>
<li><p><b><font face="Times New Roman">[03/2022]</font></b>: One paper on “Few-shot learning” is accepted to <a href="https://2022.ieeeicme.org/">ICME 2022</a>.</p>
</li>
<li><p><b><font face="Times New Roman">[09/2021]</font></b>: The code of our new work <a href="https://github.com/RL-VIG/LibFewShot.git">LibFewShot</a> <a href="https://arxiv.org/abs/2109.04898">(Paper)</a> is released.</p>
</li>
<li><p><b><font face="Times New Roman">[07/2021]</font></b>: Two papers on “Few-shot generation” and “Style transfer” are accepted to <a href="http://iccv2021.thecvf.com/home">ICCV 2021</a>.</p>
</li>
<li><p><b><font face="Times New Roman">[06/2021]</font></b>: One paper on “Meta learning” is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385">TNNLS 2021</a>.</p>
</li>
<li><p><b><font face="Times New Roman">[06/2021]</font></b>: One paper on “Caricature generation” is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046">TMM 2021</a>.</p>
</li>
<li><p><b><font face="Times New Roman">[05/2021]</font></b>: Two papers on “Few-shot learning” are accepted to <a href="https://www.journals.elsevier.com/pattern-recognition">Pattern Recognition 2021</a>.</p>
</li>
<li><p><b><font face="Times New Roman">[10/2020]</font></b>: The code of our <a href="https://github.com/WenbinLee/ADM">ADM (IJCAI'20)</a> is released.</p>
</li>
<li><p><b><font face="Times New Roman">[08/2020]</font></b>: One paper on “Caricature generation” is accepted to <a href="https://www.journals.elsevier.com/neural-networks">Neural Networks 2020</a>.</p>
</li>
<li><p><b><font face="Times New Roman">[07/2020]</font></b>: The code of our <a href="https://github.com/LegenDong/ATL-Net">ATL-Net (IJCAI'20)</a> is released.</p>
</li>
<li><p><b><font face="Times New Roman">[06/2020]</font></b>: The code of our <a href="https://github.com/xujinglin/MvNNcor">MvNNcor (AAAI'20)</a> is released.</p>
</li>
<li><p><b><font face="Times New Roman">[05/2020]</font></b>: One paper on “Few-shot learning” is early accepted to <a href="http://www.miccai.org/">MICCAI 2020</a>.</p>
</li>
<!-- <li><p><b><font face="Times New Roman">[05/2020]</font></b>: One paper on “Semi-supervised Few-shot learning” is accepted to <a href="https://2020.ieeeicip.org/">ICIP 2020</a>.</p> -->
</li>
<li><p><b><font face="Times New Roman">[04/2020]</font></b>: Four papers on “Few-shot learning”, “Multi-view learning” and “Face Recognition” are accepted to <a href="https://www.ijcai20.org/">IJCAI 2020</a>.</p>
</li>
<li><p><b><font face="Times New Roman">[11/2019]</font></b>: Successfully defending dissertation thesis on “Metric-based Few-shot Image Classification”.</p>
</li>
<li><p><b><font face="Times New Roman">[11/2019]</font></b>: Two papers on “Multi-view learning” and “Sparse coding” are accepted to <a href="https://aaai.org/Conferences/AAAI-20/">AAAI 2020</a>.</p>
</li>
<li><p><b><font face="Times New Roman">[10/2019]</font></b>: Invited talk at the third MINI Machine Learning Workshop (Nantong, China) on “Advances in Few-shot Learning”.</p>
</li>
<li><p><b><font face="Times New Roman">[04/2019]</font></b>: The code of our <a href="https://github.com/WenbinLee/DN4.git">DN4 (CVPR'19)</a> is released.</p>
</li>
<li><p><b><font face="Times New Roman">[02/2019]</font></b>: One paper on “Few-shot learning” is accepted to <a href="http://cvpr2019.thecvf.com/">CVPR 2019</a>.</p>
</li>
<li><p><b><font face="Times New Roman">[01/2019]</font></b>: The code of our <a href="https://github.com/WenbinLee/CovaMNet.git">CovaMNet (AAAI'19)</a> is released.</p>
</li>
<li><p><b><font face="Times New Roman">[11/2018]</font></b>: One paper on “Few-shot learning” is accepted to <a href="https://aaai.org/Conferences/AAAI-19/">AAAI 2019</a> .</p>
</li>
<li><p><b><font face="Times New Roman">[09/2018]</font></b>: One paper on “Deep metric learning” is accepted to <a href="http://accv2018.net/">ACCV 2018</a> .</p>
</li>
<li><p><b><font face="Times New Roman">[08/2018]</font></b>: Our caricature dataset <a href="https://cs.nju.edu.cn/rl/WebCaricature.htm">WebCaricature</a> is released.</p>
</li>
<li><p><b><font face="Times New Roman">[03/2017]</font></b>: One paper on “Online metric learning” is accepted to <a href="https://www.journals.elsevier.com/pattern-recognition"> Pattern Recognition 2018</a>.</p>
</li>
<li><p><b><font face="Times New Roman">[11/2016]</font></b>: One paper on “Metric learning” is accepted to <a href="http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14688/13946">AAAI 2017</a>.</p>
</li>
</ul>



<h2>Research Interests</h2>
<p>My research interests include <b>Machine Learning</b> and <b>Computer Vision</b>. Especially in,</p>
<ul>
<li><p>Few-shot Learning & Meta Learning</p>
</li>
<li><p>Self-supervised Learning</p>
</li>
<li><p>Adversarial Learning</p>
</li>
<li><p>Continual Learning</p>
</li>
</ul>

<h2>Preprints</h2>
<!--<h3>Conference Articles</h3>-->
</ol>



<table class="imgtable"><tbody><tr><td>
<img src="./Paper/Libfewshot.png" alt="WSFG" width="220px" height="100px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Wenbin Li</b>, Ziyi Wang, Xuesong Yang, Chuanqi Dong, Pinzhuo Tian, Tiexin Qin, Jing Huo, Yinghuan Shi, Lei Wang, Yang Gao and Jiebo Luo.<br>
 <a href="https://arxiv.org/abs/2109.04898">LibFewShot: A Comprehensive Library for Few-shot Learning.</a><br>
 <em>arXiv preprint arXiv:2109.04898</em>, 2021.<br> 
 [<a href="./Libfewshot.pdf" download="Libfewshot.pdf">Paper</a>] [<a href="https://github.com/RL-VIG/LibFewShot.git">Code</a>]
</p>
</li>
</ul>
</td></tr></tbody></table>



<table width="1000" class="imgtable"><tbody><tr><td>
<img src="./Paper/ROMA.png" alt="WSFG" width="220px" height="110px">&nbsp;</td>
<td  align="left"><ul>
<li><p><b>Wenbin Li</b>，Xuesong Yang, Meihao Kong, Lei Wang, Jing Huo, Yang Gao and Jiebo Luo.<br>
 <a href="https://arxiv.org/pdf/2107.10419.pdf">Triplet is All You Need with Random Mappings for Unsupervised Visual Representation Learning.</a><br>
 <em>arXiv preprint arXiv:2107.10419v1</em>, 2021.<br> 
 [<a href="./ROMA_arXiv.pdf" download="ROMA_arXiv.pdf">Paper</a>] 
</p>
</li>
</ul>
</td></tr></tbody></table>

</ol>



<h2>Publications (Selected)</h2>
<h3>Journal Articles:</h3>
</ol>



<table class="imgtable"><tbody><tr><td>
<img src="./Paper/MDAT.png" alt="WSFG" width="220px" height="110px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Wenbin Li</b>, Lei Wang, Xingxing Zhang, Lei Qi, Jing Huo, Yang Gao, Jiebo Luo.<br>
 <a href="https://ieeexplore.ieee.org/document/9916072">Defensive Few-shot Learning.</a><br>
 In: <em>IEEE Transactions on Pattern Analysis and Machine Intelligence <b>(TPAMI)</b></em>, 2022.<br> 
  (Impact Factor: 24.314) <br>
</p>
</li>
</ul>
</td></tr></tbody></table>



<table class="imgtable"><tbody><tr><td>
<img src="./Paper/ODML.bmp" alt="WSFG" width="220px" height="110px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Wenbin Li</b>, Yanfang Liu, Jing Huo, Yinghuan Shi, Yang Gao, Lei Wang, Jiebo Luo.<br> 
  <a href="https://ieeexplore.ieee.org/document/9928295/authors#authors">A Multilayer Framework for Online Metric Learning.</a><br>
 In: <em>IEEE Transactions on Neural Networks and Learning Systems <b>(TNNLS)</b></em>, 2022. <br> 
 (Impact Factor: 14.255) <br>
</p>
</li>
</ul>
</td></tr></tbody></table>



<table class="imgtable"><tbody><tr><td>
<img src="./Paper/CAST.png" alt="WSFG" width="220px" height="110px">&nbsp;</td>
<td align="left"><ul>
<li><p>Jing Huo, Xiangde Liu, <b>Wenbin Li</b>, Yang Gao, Hujun Yin, Jiebo Luo.<br>
 <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9765839">CAST: Learning Both Geometric and Texture Style Transfers for Effective Caricature Generation.</a><br>
 In: <em>IEEE Transactions on Image Processing <b>(TIP)</b></em>, 2022. <br> 
  (Impact Factor: 11.041) <br>
  [<a href="./CAST.pdf" download="CAST.pdf">Paper</a>] 
</p>
</li>
</ul>
</td></tr></tbody></table>



<table class="imgtable"><tbody><tr><td>
<img src="./Paper/PAA.png" alt="WSFG" width="220px" height="110px">&nbsp;</td>
<td align="left"><ul>
<li><p>Yanfang Liu, Xiaocong Fan, <b>Wenbin Li</b>, Yang Gao.<br>
 <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9791447">Online Passive-Aggressive Active Learning for Trapezoidal Data Streams.</a><br>
 In: <em>IEEE Transactions on Neural Networks and Learning Systems <b>(TNNLS)</b></em>, 2022. <br> 
 (Impact Factor: 14.255) <br>
  [<a href="./PAA.pdf" download="PAA.pdf">Paper</a>] 
</p>
</li>
</ul>
</td></tr></tbody></table>



<table class="imgtable"><tbody><tr><td>
<img src="./Paper/MetaReg.png" alt="WSFG" width="220px" height="110px">&nbsp;</td>
<td align="left"><ul>
<li><p>Pinzhuo Tian, <b>Wenbin Li</b>, Yang Gao.<br>
 <a href="https://ieeexplore.ieee.org/document/9449618">Consistent Meta-Regularization for Better Meta-Knowledge in Few-Shot Learning.</a><br>
 In: <em>IEEE Transactions on Neural Networks and Learning Systems <b>(TNNLS)</b></em>, 2021.<br> 
 (Impact Factor: 14.255) <br>
 [<a href="./TNNLS_Tian.pdf" download="TNNLS_Tian.pdf">Paper</a>] 
</p>
</li>
</ul>
</td></tr></tbody></table>



<table class="imgtable"><tbody><tr><td>
<img src="./Paper/CariMe.png" alt="WSFG" width="220px" height="110px">&nbsp;</td>
<td align="left"><ul>
<li><p>Zheng Gu, Chuanqi Dong, Jing Huo, <b>Wenbin Li</b>, Yang Gao.<br>
 <a href="https://ieeexplore.ieee.org/document/9454341">CariMe: Unpaired Caricature Generation with Multiple Exaggerations.</a><br>
 In: <em>IEEE Transactions on Multimedia <b>(TMM)</b></em>, 2021.<br> 
 (Impact Factor: 6.513) <br>
 [<a href="./TMM_GuZheng.pdf" download="TMM_GuZheng.pdf">Paper</a>] [<a href="https://github.com/edward3862/CariMe-pytorch.git">Code</a>]
</p>
</li>
</ul>
</td></tr></tbody></table>



<table class="imgtable"><tbody><tr><td>
<img src="./Paper/TemNet.png" alt="WSFG" width="220px" height="110px">&nbsp;</td>
<td align="left"><ul>
<li><p>Wei Zhu, <b>Wenbin Li</b>, Haofu Liao, Jiebo Luo.<br>
 <a href="https://www.sciencedirect.com/science/article/pii/S0031320320306002">Temperature network for few-shot learning with distribution-aware large-margin metric.</a><br>
 In: <em><b>Pattern Recognition</b></em>, 2021. <br> 
 (Impact Factor: 7.74) <br>
 [<a href="./PR_Zhu.pdf" download="PR_Zhu.pdf">Paper</a>] 
</p>
</li>
</ul>
</td></tr></tbody></table>




<table class="imgtable"><tbody><tr><td>
<img src="./Paper/LMPNet.png" alt="WSFG" width="220px" height="110px">&nbsp;</td>
<td align="left"><ul>
<li><p>Hongwei Huang, Zhangkai Wu, <b>Wenbin Li*</b>, Jing Huo*, Yang Gao.<br>
 <a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320321001229">Local descriptor-based multi-prototype network for few-shot Learning.</a><br>
 In: <em><b>Pattern Recognition</b></em>, 2021.<br> 
 (Impact Factor: 7.74) <br>
 [<a href="./PR_Huang.pdf" download="PR_Huang.pdf">Paper</a>] 
</p>
</li>
</ul>
</td></tr></tbody></table>




<table class="imgtable"><tbody><tr><td>
<img src="./Paper/CariGAN.bmp" alt="WSFG" width="220px" height="110px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Wenbin Li<sup>#</sup></b>, Wei Xiong<sup>#</sup>, Haofu Liao, Jing Huo, Yang Gao, Jiebo Luo.<br>
 <a href="https://www.sciencedirect.com/science/article/pii/S0893608020303002">CariGAN: Caricature Generation through Weakly Paired Adversarial Learning.</a><br>
 In: <em><b>Neural Networks</b></em>, 2020.<br> 
 (Impact Factor: 5.535) <br>
 [<a href="./arXiv_Cari.pdf" download="arXiv_Cari.pdf">Paper</a>] 
</p>
</li>
</ul>
</td></tr></tbody></table>



<table class="imgtable"><tbody><tr><td>
<img src="./Paper/OPML.bmp" alt="WSFG" width="220px" height="110px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Wenbin Li</b>, Yang Gao*, Lei Wang, Luping Zhou, Jing Huo, Yinghuan Shi.<br> 
<a href="http://www.sciencedirect.com/science/article/pii/S0031320317301140"> OPML: A One-Pass Closed-Form Solution for Online Metric Learning</a>. <br>
In: <b><i>Pattern Recognition</i></b>, 2018.<br> 
(Impact Factor: 7.196) <br>
[<a href="./PR18.pdf" download="PR18.pdf">Paper</a>] [<a href="https://github.com/WenbinLee/OPML.git">Code</a>]
</p> 
</li>
</ul>
</td></tr></tbody></table>


<br>
<h3>Conference Articles:</h3>
<!-- </ol> -->


<table class="imgtable"><tbody><tr><td>
<img src="./Paper/NCD.png" alt="WSFG" width="220px" height="110px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Wenbin Li</b>, Zhichen Fan, Jing Huo, Yang Gao.<br>
 <a href="https://arxiv.org/pdf/2210.03591.pdf">Modeling Inter-Class and Intra-Class Constraints in Novel Class Discovery.</a><br>
  In: <em>IEEE Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b></em>, 2023.<br> 
 (Acceptance Rate: 2360/9155=25.78%) <br>
  [<a href="https://github.com/FanZhichen/NCD-IIC.git">Code</a>]
</p>
</li>
</ul>
</td></tr></tbody></table>


<table class="imgtable"><tbody><tr><td>
<img src="./Paper/MAPPG.png" alt="WSFG" width="220px" height="110px">&nbsp;</td>
<td align="left"><ul>
<li><p>Wubing Chen, <b>Wenbin Li*</b>, Xiao Liu, Shangdong Yang, Yang Gao*.<br>
 <a>Learning Explicit Credit Assignment for Cooperative Multi-Agent Reinforcement Learning via Polarization Policy Gradient.</a><br>
  In: <em>AAAI Conference on Artificial Intelligence <b>(AAAI)</b></em>, 2023.<br> 
 (Acceptance Rate: 1721/8777=19.6%) <br>
 <!--  [<a href="./HTS_ECCV2022.pdf" download="HTS_ECCV2022.pdf">Paper</a>] [<a href="https://github.com/remiMZ/HTS-ECCV22.git">Code</a>] -->
</p>
</li>
</ul>
</td></tr></tbody></table>





<table class="imgtable"><tbody><tr><td>
<img src="./Paper/HTS.png" alt="WSFG" width="220px" height="110px">&nbsp;</td>
<td align="left"><ul>
<li><p>Min Zhang, Siteng Huang, <b>Wenbin Li</b>, Donglin Wang.<br>
 <a href="https://arxiv.org/pdf/2207.06989.pdf">Tree Structure-Aware Few-Shot Image Classification via Hierarchical Aggregation.</a><br>
 In: <em>European Conference on Computer Vision <b>(ECCV)</b></em>, 2022. <br> 
 (Acceptance Rate: 1650/5803=28%) <br>
  [<a href="./HTS_ECCV2022.pdf" download="HTS_ECCV2022.pdf">Paper</a>] [<a href="https://github.com/remiMZ/HTS-ECCV22.git">Code</a>]
</p>
</li>
</ul>
</td></tr></tbody></table>




<table class="imgtable"><tbody><tr><td>
<img src="./Paper/LoFGAN.png" alt="WSFG" width="220px" height="110px">&nbsp;</td>
<td align="left"><ul>
<li><p>Zheng Gu<sup>#</sup>, <b>Wenbin Li<sup>#</sup></b>, Jing Huo, Lei Wang, Yang Gao.<br>
 <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Gu_LoFGAN_Fusing_Local_Representations_for_Few-Shot_Image_Generation_ICCV_2021_paper.pdf">LoFGAN: Fusing Local Representations for Few-shot Image Generation.</a><br>
 In: <em>International Conference on Computer Vision <b>(ICCV)</b></em>, 2021. <br> 
  (Acceptance Rate: 1617/6236=25.9%) [# Equal contribution]<br>
  [<a href="./LoFGAN.pdf" download="LoFGAN.pdf">Paper</a>] [<a href="https://github.com/edward3862/LoFGAN-pytorch.git">Code</a>]
</p>
</li>
</ul>
</td></tr></tbody></table>




<table class="imgtable"><tbody><tr><td>
<img src="./Paper/MAST.bmp" alt="WSFG" width="220px" height="110px">&nbsp;</td>
<td align="left"><ul>
<li><p>Jing Huo, Shiyin Jin, <b>Wenbin Li*</b>, Jing Wu, Yu-Kun Lai, Yinghuan Shi, Yang Gao.<br>
 <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Huo_Manifold_Alignment_for_Semantically_Aligned_Style_Transfer_ICCV_2021_paper.pdf">Manifold Alignment for Semantically Aligned Style Transfer.</a><br>
 In: <em>International Conference on Computer Vision <b>(ICCV)</b></em>, 2021. <b>[Oral]</b><br> 
  (Acceptance Rate: 1617/6236=25.9%; Oral: 210/6236=3%) <br>
 [<a href="./MAST.pdf" download="MAST.pdf">Paper</a>] [<a href="https://github.com/NJUHuoJing/MAST.git">Code</a>]
</p>
</li>
</ul>
</td></tr></tbody></table>




<table class="imgtable"><tbody><tr><td>
<img src="./Paper/ADM.bmp" alt="WSFG" width="220px" height="110px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Wenbin Li</b>, Lei Wang, Jing Huo, Yinghuan Shi, Yang Gao, Jiebo Luo.<br>
 <a href="https://www.ijcai.org/Proceedings/2020/0409.pdf">Asymmetric Distribution Measure for Few-shot Learning.</a><br>
 In: <em>International Joint Conference on Artificial Intelligence <b>(IJCAI)</b></em>, 2020.<br> 
 (Acceptance Rate: 592/4717=12.6%) <br>
 [<a href="./ADM.pdf" download="ADM.pdf">Paper</a>] [<a href="https://github.com/WenbinLee/ADM.git">Code</a>]
</p>
</li>
</ul>
</td></tr></tbody></table>



<table class="imgtable"><tbody><tr><td>
<img src="./Paper/ATL.bmp" alt="WSFG" width="220px" height="110px">&nbsp;</td>
<td align="left"><ul>
<li><p>Chuanqi Dong, <b>Wenbin Li</b>, Jing Huo, Zheng Gu, Yang Gao.<br>
 <a href="https://www.ijcai.org/Proceedings/2020/0100.pdf">Learning Task-aware Local Representations for Few-shot Learning.</a><br> 
 In: <em>International Joint Conference on Artificial Intelligence <b>(IJCAI)</b></em>, 2020.<br> 
 (Acceptance Rate: 592/4717=12.6%) <br>
[<a href="./ATL.pdf" download="ATL.pdf">Paper</a>] [<a href="https://github.com/LegenDong/ATL-Net.git">Code</a>]
</p>
</li>
</ul>
</td></tr></tbody></table>



<table class="imgtable"><tbody><tr><td>
<img src="./Paper/MVCNN.bmp" alt="WSFG" width="220px" height="110px">&nbsp;</td>
<td align="left"><ul>
<li><p>Jinglin Xu, Xiangsen Zhang, <b>Wenbin Li</b>, Xinwang Liu, Junwei Han.<br>
 <a href="https://www.ijcai.org/Proceedings/2020/0443.pdf">Joint Multi-view 2D Convolutional Neural Networks for 3D Object Classification.</a><br>
 In: <em>International Joint Conference on Artificial Intelligence <b>(IJCAI)</b></em>, 2020.<br> 
 (Acceptance Rate: 592/4717=12.6%) <br>
 [<a href="./MVCNN.pdf" download="MVCNN.pdf">Paper</a>]
</p>
</li>
</ul>
</td></tr></tbody></table>



<table class="imgtable"><tbody><tr><td>
<img src="./Paper/BFL.bmp" alt="WSFG" width="220px" height="110px">&nbsp;</td>
<td align="left"><ul>
<li><p>Changbin Shao, Jing Huo, Lei Qi, Zhen-Hua Feng, <b>Wenbin Li</b>, Chuanqi Dong, Yang Gao.<br>
 <a href="https://www.ijcai.org/Proceedings/2020/0093.pdf">Biased Feature Learning for Occlusion Invariant Face Recognition.</a><br>
 In: <em>International Joint Conference on Artificial Intelligence <b>(IJCAI)</b></em>, 2020.<br> 
 (Acceptance Rate: 592/4717=12.6%) <br>
 [<a href="./BFL.pdf" download="BFL.pdf">Paper</a>]
</p>
</li>
</ul>
</td></tr></tbody></table>



<!-- <table class="imgtable"><tbody><tr><td>
<img src="./Paper/QR.bmp" alt="WSFG" width="220px" height="110px">&nbsp;</td>
<td align="left"><ul>
<li><p>Wei Zhu, Haofu Liao, <b>Wenbin Li</b>, Weijian Li, Jiebo Luo.<br>
 <a href="https://arxiv.org/pdf/2004.09694.pdf">Alleviating the Incompatibility between Cross Entropy Loss and Episode Training for Few-shot Skin Disease Classification.</a><br>
 In: <i>International Conference on Medical Image Computing and Computer Assisted Intervention</i> <b>(MICCAI'20)</b>, 2020. <b>[Early Accept]</b><br> 
 [<a href="./QR.pdf" download="QR.pdf">Paper</a>]
</p>
</li>
</ul>
</td></tr></tbody></table> -->



<!-- <table class="imgtable"><tbody><tr><td>
<img src="./Paper/PPSML.bmp" alt="WSFG" width="220px" height="110px">&nbsp;</td>
<td align="left"><ul>
<li><p> Pengfei Zhu, Mingqi Gu, <b>Wenbin Li</b>, Changqing Zhang, Qinghua Hu.<br>
 Progressive Point to Set Metric Learning for Semi-Supervised Few-shot Classification.<br>
 In: <i>International Conference on Image Processing</i> <b>(ICIP'20)</b>, 2020.<br> 
[<a href="./ICIP_2020.pdf" download="ICIP_2020.pdf">Paper</a>]
</p>
</li>
</ul>
</td></tr></tbody></table>
 -->




<table class="imgtable"><tbody><tr><td>
<img src="./Paper/MvNN.bmp" alt="WSFG" width="220px" height="110px">&nbsp;</td>
<td align="left"><ul>
<li><p>Jinglin Xu<sup>#</sup>, <b>Wenbin Li<sup>#</sup></b>, Xinwang Liu, Dingwen Zhang, Ji Liu, Junwei Han. <br>
<a href="https://ojs.aaai.org/index.php/AAAI/article/view/6122">Deep Embedded Complementary and Interactive Information
for Multi-view Classification.</a> <br>
 In: <em>AAAI Conference on Artificial Intelligence <b>(AAAI)</b></em>, 2020.<br> 
 (Acceptance Rate: 1591/7737=20.6%) [# Equal contribution]<br>
 [<a href="./AAAI20a.pdf" download="AAAI20a.pdf">Paper</a>] [<a href="https://github.com/xujinglin/MvNNcor">Code</a>]
 </p>
</li>
</ul>
</td></tr></tbody></table>



<table class="imgtable"><tbody><tr><td>
<img src="./Paper/LSC.bmp" alt="WSFG" width="220px" height="110px">&nbsp;</td>
<td align="left"><ul>
<li><p> Xiao Liu, <b>Wenbin Li</b>, Jing Huo, Lili Yao, Yang Gao. <br>
 <a href="https://aaai.org/ojs/index.php/AAAI/article/view/5927/5783">Layerwise Sparse Coding for Pruned Deep Neural Networks with Extreme Compression Ratio.</a><br>
 In: <em>AAAI Conference on Artificial Intelligence <b>(AAAI)</b></em>, 2020.<br> 
 (Acceptance Rate: 1591/7737=20.6%) <br>
 [<a href="./LSC.pdf" download="LSC.pdf">Paper</a>] 
 </p>
</li>
</ul>
</td></tr></tbody></table>


<table class="imgtable"><tbody><tr><td>
<img src="./Paper/DN4.bmp" alt="WSFG" width="220px" height="110px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Wenbin Li</b>, Lei Wang, Jinglin Xu, Jing Huo, Yang Gao, Jiebo Luo. <br>
 <a  href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Revisiting_Local_Descriptor_Based_Image-To-Class_Measure_for_Few-Shot_Learning_CVPR_2019_paper.pdf"> Revisiting Local Descriptor based Image-to-Class Measure for Few-shot Learning.</a> <br>
 In: <em>IEEE Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b></em>, 2019.<br> 
 (Acceptance Rate: 1299/5165=25.2%) <br>
 [<a href="./CVPR19.pdf" download="CVPR19.pdf">Paper</a>] [<a href="https://github.com/WenbinLee/DN4.git">Code</a>]
 </p>
</li>
</ul>
</td></tr></tbody></table>



<table class="imgtable"><tbody><tr><td>
<img src="./Paper/CovaMNet.bmp" alt="WSFG" width="220px" height="110px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Wenbin Li<sup>#</sup></b>, Jinglin Xu<sup>#</sup>, Jing Huo, Lei Wang, Yang Gao, Jiebo Luo. <br>
 <a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4885/4758">Distribution Consistency based Covariance Metric Networks for Few-shot Learning</a>.<br>
 In: <em>AAAI Conference on Artificial Intelligence <b>(AAAI)</b></em>, 2019. <b>[Oral]</b><br> 
 (Acceptance Rate: 1150/7095=16.2%) [# Equal contribution]<br>
 [<a href="./AAAI19.pdf" download="AAAI19.pdf">Paper</a>] [<a href="./AAAI19_slide.pdf" download="AAAI19_slide.pdf">Slides</a>] [<a href="https://github.com/WenbinLee/CovaMNet.git">Code</a>]
 </p>
</li>
</ul>
</td></tr></tbody></table>


<!-- 
<table class="imgtable"><tbody><tr><td>
<img src="./Paper/LGDML.bmp" alt="WSFG" width="220px" height="110px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Wenbin Li</b>, Jing Huo, Yinghuan Shi, Yang Gao, Lei Wang, Jiebo Luo. <br>
 <a href="https://link.springer.com/content/pdf/10.1007%2F978-3-030-20870-7_15.pdf">A Joint Local and Global Deep Metric Learning Method for Caricature Recognition</a>.<br>
 In: <i>Asian Conference on Computer Vision</i> <b>(ACCV'18)</b>, 2018.<br> 
 (Acceptance Rate: 274/979=27.98%) <br>
 [<a href="./ACCV18.pdf" download="ACCV18.pdf">Paper</a>] [<a href="./ACCV18_Poster.pdf" download="ACCV18_Poster.pdf">Poster</a>]
 </p>
</li>
</ul>
</td></tr></tbody></table> -->




<!-- <table class="imgtable"><tbody><tr><td>
<img src="./Paper/CaricatureJin.bmp" alt="WSFG" width="220px" height="110px">&nbsp;</td>
<td align="left"><ul>
<li><p>Jing Huo, <b>Wenbin Li</b>, Yinghuan Shi, Yang Gao, Hujun Yin. <br>
 <a href="https://arxiv.org/pdf/1703.03230.pdf"> WebCaricature: a benchmark for caricature recognition</a>. <br>
 In: <i>British Machine Vision Conference</i> <b>(BMVC'18)</b>, 2018.<br> 
 (Acceptance Rate: 255/862=29.5%) <br>
 [<a href="./BMVC18.pdf" download="BMVC18.pdf">Paper</a>] [<a href="https://cs.nju.edu.cn/rl/WebCaricature.htm">Data</a>]
 </p> 
</li>
</ul>
</td></tr></tbody></table> -->


<!-- <table class="imgtable"><tbody><tr><td>
<img src="./Paper/MSSN_ICPR.bmp" alt="WSFG" width="220px" height="110px">&nbsp;</td>
<td align="left"><ul>
<li><p>Minglin Ma, Yinghuan Shi, <b>Wenbin Li</b>, Yang Gao, Jun Xu. <br>
<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8546192"> A Novel Two-Stage Deep Method for Mitosis Detection in Breast Cancer Histology Images</a>.<br>
 In: <i> International Conference on Pattern Recognition</i> <b>(ICPR'18)</b>, 2018.<br> 
 [<a href="./ICPR18.pdf" download="ICPR18.pdf">Paper</a>] 
 </p>
</li>
</ul>
</td></tr></tbody></table> -->


<table class="imgtable"><tbody><tr><td>
<img src="./Paper/NoniidML.bmp" alt="WSFG" width="220px" height="110px">&nbsp;</td>
<td align="left"><ul>
<li><p>Yinghuan Shi, <b>Wenbin Li</b>, Yang Gao, Longbing Cao, Dinggang Shen.<br>
 <a href="https://ojs.aaai.org/index.php/AAAI/article/view/10748"> Beyond IID: Learning to Combine Non-IID Metrics for Vision Tasks</a>. <br>
 In: <em>AAAI Conference on Artificial Intelligence <b>(AAAI)</b></em>, 2017.<br> 
  (Acceptance Rate: 638/2590=24.6%) <br>
 [<a href="./AAAI17.pdf" download="AAAI17.pdf">Paper</a>]
 </p>
</li>
</ul>
</td></tr></tbody></table>



<!-- <table class="imgtable"><tbody><tr><td>
<img src="./Paper/CascadedML.bmp" alt="WSFG" width="220px" height="110px">&nbsp;</td>
<td align="left"><ul>
<li><p><b>Wenbin Li</b>, Yinghuan Shi, Wanqi Yang, Hao Wang, Yang Gao. <br>
 <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7351333"> Interactive Image Segmentation Via Cascaded Metric Learning.</a> <br>
 In: <i>International Conference on Image Processing</i> <b>(ICIP'15)</b>, 2015.<br> 
 [<a href="./ICIP15.pdf" download="ICIP15.pdf">Paper</a>]
</p>
</li>
</ul>
</td></tr></tbody></table> -->

</ol>


<h2>Datasets & Framework</h2>
<ul>
<table class="imgtable">
  <tbody>
    <tr>
    <td>
        <img src="./Paper/Libfewshot2.png" width="250">
      </td>
      <td width="500"  style="vertical-align:text-top;">
        <h3><a href="https://github.com/RL-VIG/LibFewShot.git"><b>LibFewShot</b></a><br></h3>
        <p style=text-align:justify>LibFewShot is a comprehensive library for few-shot learning, especially for few-shot image classification. It integrates multiple classic FSL methods into a unified framework, including four fine-tuning based methods, six meta-learning based methods, and eight metric-learning based methods.</td>
    </p>
    </tr>
  </tbody>
</table>
<table class="imgtable">
  <tbody>
    <tr>
	  <td>
        <img src="./Paper/CaricatureJin.bmp" width="250">
      </td>
      <td width="500"  style="vertical-align:text-top;">
        <h3><a href="https://cs.nju.edu.cn/rl/WebCaricature.htm"><b>WebCaricature Dataset</b></a><br></h3>
        <p style=text-align:justify> WebCaricature dataset is developed for caricature recognition. 
    It is a large photograph-caricature dataset consisting of 6042 caricatures and 5974 photographs from 252 persons collected from the web</td>
		</p>
    </tr>
  </tbody>
</table>
</ul>


<h2>Awards & Honors</h2>
<ul>
<li><p><font face="华文楷体">2021 江苏省双创博士</font></p>
</li>
<li><p><font face="华文楷体">2021 南京大学紫金学者</font></p>
</li>
<li><p><font face="华文楷体">2020 华为杯第二届中国研究生人工智能创新大赛优秀指导教师</font></p>
</li>
<li><p><font face="华文楷体">2020 江苏省计算机学会优秀博士论文奖</font></p>
</li>
</ul>


<h2>Correspondence</h2>
<p>Email:
<a href="mailto:liwenbin@nju.edu.cn">liwenbin@nju.edu.cn</a>; <a href="mailto:liwenbin.nju@gmail.com">liwenbin.nju@gmail.com</a><br>
Office:
Room 1019, Computer Science Building, Xianlin Campus of Nanjing University.<br>
Address:
Wenbin Li, State Key Laboratory for Novel Software Technology, Nanjing University, Xianlin Campus, 163 Xianlin Avenue,<br>
Qixia District, Nanjing 210023, China<br>
<b><font face="华文楷体">(南京市栖霞区仙林大道163号, 南京大学仙林校区, 软件新技术国家重点实验室, 210023.)</font></b></p>
<div id="footer">
<!--<div id="footer-text">
Page updated on 2015.12.2, via <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>-->
</div>
</div>
<a href='https://clustrmaps.com/site/1bdml'  title='Visit tracker'><img src='//clustrmaps.com/map_v2.png?cl=ffffff&w=349&t=tt&d=SXzcVa_nPEO7nuVNB3l1W3DGKNwUvnH-lNkZYlHLhKc'/></a>

</body></html>